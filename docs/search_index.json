[["index.html", "Szövegbányászat és mesterséges intelligencia R-ben Üdvözöljük!", " Szövegbányászat és mesterséges intelligencia R-ben Sebk Miklós, Ring Orsolya, Máté Ákos 2021 Üdvözöljük! A tankonyv javitott verziojanak roviditett indexe. "],["alapfogalmak.html", "1 Alapfogalmak 1.1 Elméleti alapok 1.2 Fogalmi alapok 1.3 A szövegbányászat alapelvei", " 1 Alapfogalmak 1.1 Elméleti alapok A szövegek géppel való feldolgozása és elemzése módszertanának számos megnevezése létezik. A szövegelemzés, kvantitatív szövegelemzés, szövegbányászat, természetes nyelvfeldolgozás, automatizált szövegelemzés, automatizált tartalomelemzés és hasonló fogalmak között nincs éles tartalami különbség. Ezek a kifejezések jellemzen ugyanarra az általánosabb kutatási irányra reflektálnak, csupán hangsúlybeli eltolódások vannak köztük, így gyakran szinonimaként is használják ket. A szövegek gépi feldolgozásával foglalkozó tudományág a Big Data forradalom részeként kezdett kialakulni, melyet az adatok egyre nagyobb és diverzebb tömegének elérhet és összegyjthet jellege hívott életre. Ennek megfelelen az adattudomány számos különböz adatforrás, így képek, videók, hanganyagok, internetes keresési adatok, telefonok lokációs adatai és megannyi különböz információ feldolgozásával foglalkozik. A szöveg is egy az adatbányászat érdekldési körébe tartozó számos adattípus közül, melynek elemzésére külön kutatási irány alakult ki. Mivel napjainkban minden másodpercben óriási mennyiség szöveg keletkezik és válik hozzáférhetvé az interneten, egyre nagyobb az igény az ilyen jelleg források és az emberi nyelv automatizált feldolgozására. Ebbl adódóan az elemzési eszköztár is egyre szélesebb kör és egyre szofisztikáltabb, így a tartalomelemzési és szövegbányászati ismeretekkel bíró elemzk számára rengeteg értékes információ kinyerhet. Ezért a szövegbányászat nemcsak a társadalomtudósok számára izgalmas kutatási irány, hanem gyakran hasznosítják üzleti célokra is. Gondoljunk például az online sajtótermékekre, az ezekhez kapcsolódó kommentekre vagy a politikusok beszédéire. Ezek mind-mind hatalmas mennyiségben rendelkezésre állnak, hasznosításukhoz azonban képesnek kell lenni ezeket a szövegeket összegyjteni, megfelel módon feldolgozni és kiértékelni. A könyv ebben is segítséget nyújt az Olvasónak. Mieltt azonban az adatkezelés és az elemzés részleteire rátérnénk, érdemes végigvenni néhány elvi megfontolást, melyek nélkülözhetetlenek a leend elemz számára az etikus, érvényes és eredményes szövegbányászati kutatások kivitelezéséhez. A nagy mennyiségben rendelkezésre álló szöveges források kiváló kutatási terepet kínálnak a társadalomtudósok számára megannyi vizsgálati kérdéshez, azonban fontos tisztában lenni azzal, hogy a mindenki által elérhet adatokat is meglehetsen körültekinten, etikai szempontok figyelembevételével kell használni. Egy másik szempont, amelyet érdemes szem eltt tartani, mieltt az ember fejest ugrana az adatok végtelenjébe, a 3V elve: volume, velocity, variety vagyis az adatok mérete, a keletkezésük sebessége és azok változatossága (Brady 2019). Ezek mind olyan tulajdonságok, amelyek az adatelemzt munkája során más (és sok esetben több vagy nagyobb) kihívások elé állítják, mint egy hagyományos statisztikai elemzés esetében. A szövegbányászati módszerek abban is eltérnek a hagyományos társadalomtudományi elemzésektl, hogy  az adattudományokba visszanyúló gyökerei miatt  jelents teret nyit az induktív (empiricista) kutatások számára a deduktív szemlélettel szemben. A deduktív kutatásmódszertani megközelítés esetén a kutató elre meghatározza az alkalmazandó fogalomrendszert, és azokat az elvárásokat, amelyek teljesülése esetén sikeresnek tekinti az elemzést. Az adattudományban az ilyen megközelítés a felügyelt tanulási feladatokat jellemzi, vagyis azokat a feladatokat, ahol ismert az elvárt eredmény. Ilyen például egy osztályozási feladat, amikor újságcikkeket szeretnénk különböz témakörökbe besorolni. Ebben az esetben az adatok egy részét általában kézzel kategorizáljuk, és a gépi eljárás sikerességét ehhez viszonyítjuk. Mivel az ideális eredmény (osztálycímke) ismert, a gépi teljesítmény könnyen mérhet (például a pontosság, a gép által sikeresen kategorizált cikkek százalékában kifejezve). Az induktív megoldás esetében kevésbé egyértelm a gépi eljárás teljesítményének mérése, hiszen a rejtett mintázatok feltárását várjuk az algoritmustól, emiatt nincsenek elre meghatározott eredmények sem, amelyekhez viszonyíthatjuk a teljesítményt. Az adattudományban az ilyen feladatokat hívják felügyelet nélküli tanulásnak. Ide tartozik a klaszterelemzés, vagy a topic modellezés, melynek esetén a kutató csak azt határozza meg, hány klasztert, hány témát szeretne kinyerni, a gép pedig létrehozza az egymáshoz leghasonlóbb csoportokat. Értelemszeren itt a kutatói validálás jóval nagyobb hangsúlyt kap, mint a deduktív megközelítés esetében. Egy harmadik, középutas megoldás a megalapozott elmélet megközelítése, mely ötvözi az induktív és a deduktív módszer elnyeit. Ennek során a kutató kidolgoz egy laza elméleti keretet, melynek alapján elvégzi az elemzést, majd az eredményeket figyelembe véve finomít a fogalmi keretén, és újabb elemzést futtat, addig folytatva ezt az iterációt, amíg a kutatás eredményeit kielégítnek nem találja. A szövegbányászati elemzéseket kategorizálhatjuk továbbá a gépi hozzájárulás mértéke szerint. Ennek megfelelen megkülönböztethetünk kézi, géppel támogatott és gépi eljárásokat. Mindhárom megközelítésnek megvan a maga elnye. A kézi megoldások esetén valószínbb, hogy azt mérjük a szövegünkben, amit mérni szeretnénk (például bizonyos szakpolitikai tartalmat), ugyanakkor ez id- és költségigényes. A gépi eljárások ezzel szemben költséghatékonyak és gyorsak, de fennáll a veszélye, hogy nem azt mérjük, amit eredetileg mérni szerettünk volna (ennek megállapításában ismét a validálás kap kulcsszerepet). Továbbá lehetséges kézzel támogatott gépi megoldások alkalmazása, ahol a humán és a gépi elemzés ideális arányának megtalálása jelenti a f kihívást. 1.2 Fogalmi alapok Miután áttekintettük a szövegbányászatban használatos elméleti megközelítéseket, érdemes tisztázni a fogalmi alapokat is. A szövegbányászat szempontjából a szöveg is egy adat. Az elemzéshez használatos strukturált adathalmazt korpusznak nevezzük. A korpusz az összes szövegünket jelöli, ennek részegységei a dokumentumok. Ha például a Magyar Nemzet cikkeit kívánjuk elemezni, a kiválasztott idszak összes cikke lesz a teljes korpuszunk, az egyes cikkek pedig a dokumentumaink. Az elemzés mindig egy meghatározott tématerületre (domain-re) koncentrál. E tématerület utalhat a nyelvre, amelyen a szövegek íródtak, vagy a specifikus tartalomra, amelyet vizsgálunk, de mindenképpen meghatározza a szöveg szókészletével kapcsolatos várakozásainkat. Más lesz tehát a szóhasználat egy bulvárlap cikkeiben, mint egy tudományos szaklap cikkeiben, aminek elssorban akkor van jelentsége, ha szótár alapú elemzéseket készítünk. A szótár alapú elemzések során olyan szószedeteket hozunk létre, amelyek segíthetnek a kutatásunk szempontjából érdekes témák vagy tartalmak azonosításában. Így például létrehozhatunk pozitív és negatív szótárakat, vagy a gazdasági és a külpolitikai témákhoz kapcsolódó szótárakat, melyek segíthetnek azonosítani, hogy adott dokumentum inkább gazdasági vagy inkább külpolitikai témákat tárgyal. Léteznek elre elkészített szótárak  angol nyelven például a Bing Liu által fejlesztett szótár egy jól ismert és széles körben alkalmazható példa (Liu 2010) , azonban fontos fejben tartani, hogy a vizsgált téma specifikus nyelvezete jellemzen meghatározza azt, hogy egy-egy szótárba milyen kifejezéseknek kellene kerülniük. Már említettük, hogy egy szövegbányászati elemzés során a szöveg is adatként kezelend. Tehát hasonló módon gondolhatunk az elemzend szövegeinkre, mint egy statisztikai elemzésre szánt adatbázisra, annak csupán, reprezentációja tér el az utóbbitól. Tehát míg egy statisztikai elemzésre szánt táblázatban elssorban számokat és adott esetben kategorikus változókat reprezentáló karakterláncokat (stringeket)  például férfi/n, falu/város  találunk, addig a szöveges adatokban els ránézésre nem tnik ki gépileg értelmezhet struktúra. Ahhoz, hogy a szövegeink a gépi elemzés számára feldolgozhatóvá váljanak, annak reprezentációját kell megváltoztatni, vagyis strukturálatlan adathalmazból strukturált adathalmazt kell létrehozni, melyet jellemzen a szövegek mátrixszá alakításával teszünk meg. A mátrixszá alakítás els hallásra bonyolult eljárás benyomását keltheti, azonban a gyakorlatban egy meglehetsen egyszer transzformációról van szó, melynek eredményeként a szavakat számokkal reprezentáljuk. A könnyebb megértés érdekében vegyük az alábbi példát: tekintsük a három példamondatot a három elemzend dokumentumnak, ezek összességét pedig a korpuszunknak. 1. Az Európai Unió 27 tagországának egyike Magyarország. 2. Magyarország 2004-ben csatlakozott az Európai Unióhoz. 3. Szlovákia, akárcsak Magyarország, 2004-ben lett ez Európai Unió tagja. A példamondatok dokumentum-kifejezés mátrixsza az alábbi táblázat szerint fog kinézni. Vegyük észre azt is, hogy több olyan kifejezés van, melyek csak ragozásukban térnek el: Unió, Unióhoz; tagja, tagjának. Ezeket a kifejezéseket a kutatói szándék függvényében azonos alakúra hozhatjuk, hogy egy egységként jelenjenek meg. Az elemzések többségében a szövegelkészítés egyik kiinduló lépése a szótövesítés vagy a lemmatizálás, elbbi a szavak toldalékainak levágását jelöli, utóbbi a szavak szótári alakra való visszaalakítását. A ragozás eltávolítását illeten elöljáróban annyit érdemes megjegyezni, hogy az agglutináló, vagyis ragasztó nyelvek esetén, mint amilyen a magyar is, a toldalékok eltávolítása gyakran igen komoly kihívást jelent. Nem csak a toldalékok formája lehet igen sokféle, de az is elfordulhat, hogy a tszó nem egyezik meg a toldalék levágásával keletkez szótvel. Ilyen például a vödröt kifejezés, melynek szótöve a vödr, de a nyelvtanilag helyes tszó a vödör. Hasonlóan a majmok kifejezés esetén a szót a majm lesz, míg a nyelvtanilag helyes tszó a majom. Emiatt a toldalékok levágását a magyar nyelv szövegek esetén megfelel körültekintéssel kell végezni. Táblázat 1.1: Dokumentum-kifejezés mátrix három példamondattal Dokumentum száma 27 2004-ben akárcsak az csatlakozott egyike Európai lett Magyarország Szlovákia tagja tagjának Unió Unióhoz 1 1 0 0 1 0 1 1 0 1 0 0 1 1 0 2 0 1 0 1 1 0 1 0 1 0 0 0 0 1 3 0 1 1 1 0 0 1 1 1 1 1 0 1 0 A dokumentum-kifejezés mátrixban minden dokumentumot egy vektor (értsd: egy sor) reprezentál, az eltér kifejezések pedig külön oszlopokat kapnak. Tehát a fenti példában minden dokumentumunk egy 14 elem vektorként jelenik meg, amelynek elemei azt jelölik, hogy milyen gyakran szerepel az adott kifejezés a dokumentumban. A dokumentum-kifejezés mátrixok egy jellemz tulajdonsága, hogy igen nagy dimenziókkal rendelkezhetnek (értsd: sok sorral és sok oszloppal), hiszen minden kifejezést külön oszlopként reprezentálnak. Egy sok dokumentumból álló vagy egy témák tekintetében változatos korpusz esetében a kifejezés mátrix elemeinek jelents része 0 lesz, hiszen számos olyan kifejezés fordul el az egyes dokumentumokban, amelyek más dokumentumban nem szerepelnek. A sok nullát tartalmazó mátrixot hívjuk ritka mátrixnak. Az adatok jobb kezelhetsége érdekben a ritka mátrixot valamilyen dimenzióredukciós eljárással sr mátrixszá lehet alakítani (például a nagyon ritka kifejezések eltávolításával vagy valamilyen súlyozáson alapuló eljárással). 1.3 A szövegbányászat alapelvei A módszertani fogalmak tisztázásást követen néhány elméleti megfontolást osztanánk meg a Grimmer és Steward (2013) által megfogalmazott alapelvek nyomán, melyek hasznos útravalóul szolgálhatnak a szövegbányászattal ismerked kutatók számára. 1. A szövegbányászat rossz, de hasznos Az emberi nyelv egy meglehetsen bonyolult rendszer, így egy szöveg jelentésének, érzelmi telítettségének különböz olvasók általi értelmezése meglehetsen eltér lehet, így nem meglep, hogy egy gép sok esetben csak korlátozott eredményeket képes felmutatni ezen feladatok teljesítésében. Ettl függetlenül nem elvitatható a szövegbányászati modellek hasznossága, hiszen olyan mennyiség szöveg válik feldolgozhatóvá, ami gépi támogatás nélkül elképzelhetetlen lenne, mindemellett azonban nem lehet megfeledkezni a módszertan korlátairól sem. 2. A kvantitatív modellek kiegészítik az embert, nem helyettesítik azt A kvantitatív eszközökkel történ elemzés nem szünteti meg a szövegek elolvasásának szükségességét, hiszen egészen más információk kinyerését teszi lehetvé, mint egy kvalitatív megközelítés. Emiatt a kvantitatív szövegelemzés talán legfontosabb kihívása, hogy a kutató megtalálja a gépi és a humán erforrások együttes hasznosításának legjobb módját. 3. Nincs legjobb modell Minden kutatáshoz meg kell találni a leginkább alkalmas modellt a kutatási kérdés, a rendelkezésre álló adatok és a kutatói szándék alapján. Gyakran különböz eljárások kombinálása vezethet egy specifikus probléma legjobb megoldásához. Azonban minden esetben az eredmények értékelésére kell támaszkodni, hogy megállapíthassuk egy modell teljesítményét adott problémára és szövegkorpuszra nézve. 4. Validálás, validálás, validálás! Mivel az automatizált szövegelemzés számos esetben jelentsen lecsökkenti az elemzéshez szükséges idt és energiát, csábító lehet a gondolat, hogy ezekhez a módszerekhez forduljon a kutató, ugyanakkor nem szabad elfelejteni, hogy az elemzés csupán a kezdeti lépés, hiszen a kutatónak validálnia kell az eredményeket ahhoz, hogy valóban megbízható következtetésekre jussunk. Az érvényesség-vizsgálat (validálás) lényege egy felügyelet nélküli modell esetén  ahol az elvárt eredmények nem ismertek, így a teljesítmény nem tesztelhet , hogy meggyzdjünk: egy felügyelt modellel (olyan modellel, ahol az elvárt eredmény ismert, így ellenrizhet) egyenérték eredményeket hozzon. Ennek az elvárásnak a teljesítése gyakran nem egyszer, azonban az eljárások alapos kiértékelést nélkülöz alkalmazása meglehetsen kétesélyes eredményekhez vezethet, emiatt érdemes megfelel alapossággal eljárni az érvényesség-vizsgálat során. "]]
